{
  "copyright_text": "",
  "description": "As Large Language Models (LLMs) gain trust across various sectors for tasks ranging from generating text to solving complex queries, their influence continues to expand. Yet, this trust is shadowed by significant risks, such as the subtle yet serious threat of data poisoning. This talk will delve into how deceptively crafted data can infiltrate an LLM\u2019s training set, leading these models to propagate errors, biases, or outright fabrications\u2014a real challenge to the integrity of their outputs.\n\nWhile there are various algorithms and approaches designed to mitigate these risks, this session will focus particularly on the Rank-One Model Editing (ROME) algorithm. ROME is notable for its ability to edit an LLM's knowledge in a targeted manner after training, providing a means to recalibrate AI outputs. However, it also presents a potential for misuse, as it can be employed to embed false narratives deeply within a model.\n\nKey Discussion Points:\n- **Why People Trust LLMs**: Exploring the reasons behind the widespread trust in LLMs and the associated risks.\n- **The Art of Data Poisoning**: A closer look at how maliciously crafted data is inserted into training sets and its profound impact on model behavior.\n- **Focus on ROME**: Discussing how the Rank-One Model Editing algorithm can both safeguard against and potentially contribute to the corruption of LLMs.\n- **Ethical Considerations**: Reflecting on the ethical implications of manipulating the knowledge within LLMs, which requires not just technical skill but also wisdom and responsibility.\n\nThis presentation is designed for data scientists, AI researchers, and Python enthusiasts interested in understanding the vulnerabilities of LLMs and the tools available to protect these systems. While acknowledging other algorithms and methods, this talk will provide a quick demonstration of ROME, offering insights into its utility and dangers.\n\nAs people continue to integrate LLMs into everything, we must remain vigilant against the risks of data manipulation. This session challenges us to consider whether we are paying enough attention to these threats, or if we are, metaphorically, just fiddling while Rome burns\u2014allowing foundational trust in data to erode. \n\nJoin me in this exploration of ROME, where we navigate the fine balance between correcting and corrupting the digital minds that are\u2014whether we like it or not\u2014becoming an integral part of our technological landscape.\n",
  "duration": 1319,
  "language": "eng",
  "recorded": "2024-05-29",
  "related_urls": [
    {
      "label": "Conference schedule",
      "url": "https://2024.northbaypython.org/"
    },
    {
      "label": "Full playlist",
      "url": "https://www.youtube.com/playlist?list=PLaeNpBNgqQWu1WqbrERc3MyrT7jarNM_n"
    }
  ],
  "speakers": [
    "Paris Buttfield-Addison"
  ],
  "tags": [],
  "thumbnail_url": "https://i.ytimg.com/vi/t5cFHvWKPD8/maxresdefault.jpg",
  "title": "Nightmare on LLM Street: The Perils and Paradoxes of Knowing Your Foe",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=t5cFHvWKPD8"
    }
  ]
}
