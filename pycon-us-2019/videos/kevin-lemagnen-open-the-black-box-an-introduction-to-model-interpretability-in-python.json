{
  "copyright_text": null,
  "description": "What's the use of sophisticated machine learning models if you can't\ninterpret them?\n\nIn fact, many industries including finance and healthcare require clear\nexplanations of why a decision is made. This tutorial covers recent\nmodel interpretability techniques that are essentials in your data\nscientist toolbox: Eli5, LIME (Local Interpretable Model-Agnostic\nExplanations) and SHAP (SHapley Additive exPlanations).\n\nYou will learn how to apply these techniques in Python on real-world\ndata science problems in order to debug your models and explain their\ndecisions.\n\nYou will also learn the conceptual background behind these techniques so\nyou can better understand when they are appropriate.\n",
  "duration": 13102,
  "language": "eng",
  "recorded": "2019-05-02T13:20:00",
  "related_urls": [
    {
      "label": "Conference schedule",
      "url": "https://us.pycon.org/2019/schedule/talks/"
    },
    {
      "label": "Conference slides (speakerdeck)",
      "url": "https://speakerdeck.com/pycon2019"
    },
    {
      "label": "Conference slides (github)",
      "url": "https://github.com/PyCon/2019-slides"
    },
    {
      "label": "Talk schedule",
      "url": "https://us.pycon.org/2019/schedule/presentation/88/"
    }
  ],
  "speakers": [
    "Kevin Lemagnen"
  ],
  "tags": [
    "tutorial"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/d66ttkMY9zE/maxresdefault.jpg",
  "title": "Open the Black Box: an Introduction to Model Interpretability in Python",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=d66ttkMY9zE"
    }
  ]
}
