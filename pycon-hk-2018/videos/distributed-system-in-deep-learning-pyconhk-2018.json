{
  "copyright_text": "Creative Commons Attribution license (reuse allowed)",
  "description": "The recent trends in distributed Deep Learning and how distributed\nsystem can both massively reduce training time and enable\nparallelisation. I will introduce different distributed deep learning\nparadigms, including model-level parallelism and data-level parallelism,\nand demonstrate how data parallelism can be used for distributed\ntraining.In next iteration phase of deep learning there will be need of\ndistributed GPU computation.\n\nAs data volumes increase, GPU clusters will be needed for the new\ndistributed methods that already produce produce the state-of-the-art\nresults for ImageNet and Cifar-10, such as neural architecture search.\nAuto-ml is also predicated on the availability of GPU clusters. Deep\nLearning systems at hyper-scale AI companies attack the toughest\nproblems with distributed deep learning. Distributed Deep Learning\nenables both AI researchers and practitioners to be more productive and\nthe training of models that would be intractable on a single GPU server.\n",
  "duration": 1186,
  "language": "eng",
  "recorded": "2018-11-23",
  "related_urls": [
    {
      "label": "Conference schedule",
      "url": "http://pycon.hk/schedule-2018/"
    }
  ],
  "speakers": [
    "Ashutosh Singh"
  ],
  "tags": [],
  "thumbnail_url": "https://i.ytimg.com/vi/lSVNkRWUHIQ/maxresdefault.jpg",
  "title": "Distributed System in Deep Learning",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=lSVNkRWUHIQ"
    }
  ]
}
