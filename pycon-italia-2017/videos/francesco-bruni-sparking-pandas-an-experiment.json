{
  "description": "Pandas is a good library to deal with tabular data. What if you need to\nmanage an amount of data that doesn\u2019t fit into memory? What if you want\nto \u201cdistribute\u201d your computations among multiple machines?\n\nStarting from a real scenario, Apache Spark will be presented as the\nmain tool to read and process collected data. It will be shown how a\nPandas-like syntax will come in handy to run aggregations, filtering and\ngrouping using a Spark Dataframe.\n\nA previous knowledge of Docker and Docker Compose will be very useful\nwhile knowing MongoDB (where data will be fetched from) is not\nmandatory. Basics of functional programming will help to understand\nSpark inner logic.\n",
  "duration": 1735,
  "language": "eng",
  "recorded": "2017-04-07",
  "related_urls": [],
  "speakers": [
    "Francesco Bruni"
  ],
  "tags": [
    "microservices",
    "Jupyter",
    "mongodb",
    "data-visualization",
    "data-analysis",
    "spark",
    "docker"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/rcGBljYexwI/hqdefault.jpg",
  "title": "Sparking Pandas: an experiment",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=rcGBljYexwI"
    }
  ]
}
