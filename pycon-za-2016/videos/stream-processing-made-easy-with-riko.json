{
  "description": "AUDIENCE\n========\n\n-  data scientists (current and aspiring)\n-  those who want to know more about data processing\n-  those who are intimidate by \"big data\" (java) frameworks and are\n   interested in a simpler, pure python alternative\n-  those interested in async and/or parallel programming\n\nDESCRIPTION\n===========\n\nBig data processing is all the rage these days. Heavyweight frameworks\nsuch as Spark, Storm, Kafka, Samza, and Flink have taken the spotlight\ndespite their complex setup, java dependency, and intense computer\nresource usage.\n\nThose interested in simple, pure python solutions have limited options.\nMost alternative software is synchronous, doesn't perform well on large\ndata sets, or is poorly documented.\n\nThis talk aims to explain stream processing and its uses, and introduce\nriko: a pure python stream processing library built with simplicity in\nmind. Complete with various examples, you\u2019ll get to see how riko lazily\nprocesses streams via its synchronous, asynchronous, and parallel\nprocessing APIs.\n\nOBJECTIVES\n==========\n\nAttendees will learn what streams are, how to process them, and the\nbenefits of stream processing. They will also see that most data isn't\n\"big data\" and therefore doesn't require complex (java) systems\n(\\**cough\\** spark and storm \\*\\ *cough\\**) to process it.\n\nDETAILED ABSTRACT\n=================\n\nStream processing?\n------------------\n\nWhat are streams?\n~~~~~~~~~~~~~~~~~\n\nA stream is a sequence of data. The sequence can be as simple as a list\nof integers or as complex as a generator of dictionaries.\n\nHow do you process streams?\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nStream processing is the act of taking a data stream through a series of\noperations that apply a (usually pure) function to each element in the\nstream. These operations are pipelined so that the output of one\nfunction is the input of the next one. By using pure functions, the\nprocessing becomes embarrassingly parallel: you can split the items of\nthe stream into separate processes (or threads) which then perform the\noperations simultaneously (without the need for communicating between\nprocesses/threads). [1-4]\n\nWhat can stream processing do?\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nStream processing allows you to efficiently manipulate large data sets.\nThrough the use of lazy evaluation, you can process data stream too\nlarge to fit into memory all at once.\n\nAdditionally, stream processing has several real world applications\nincluding:\n\n-  parsing rss feeds (rss readers, think\n   `feedly <http://feedly.com/>`__)\n-  combining different types data from multiple sources in innovative\n   ways (mashups, think `trendsmap <http://trendsmap.com/>`__)\n-  taking data from multiple sources, manipulating the data into a\n   homogeneous structure, and storing the result in a database\n   (extracting, transforming, and loading data; aka ETL, data\n   wrangling...)\n-  aggregating similarly structured data from siloed sources and\n   presenting it via a unified interface (aggregators, think\n   `kayak <kayak.com>`__)\n\n[5, 6]\n\nStream processing frameworks\n----------------------------\n\nIf you've heard anything about stream processing, chances are you've\nalso heard about frameworks such as Spark, Storm, Kafka, Samza, and\nFlink. While popular, these frameworks have a complex setup and\ninstallation process, and are usually overkill for the amount of data\ntypical python users deal with. Using a few examples, I will show basic\nStorm usage and how it stacks up against BASH.\n\nIntroducing riko\n----------------\n\nSupporting both Python 2 and 3, riko is the first pure python stream\nprocessing library to support synchronous, asynchronous, and parallel\nprocessing. It's built using functional programming methodology and lazy\nevaluation by default.\n\nBasic riko usage\n~~~~~~~~~~~~~~~~\n\nUsing a series of examples, I will show basic riko usage. Examples will\ninclude counting words, fetching streams, and rss feed manipulation. I\nwill highlight the key features which make riko a better stream\nprocessing alternative to Storm and the like.\n\nriko's many paradigms\n~~~~~~~~~~~~~~~~~~~~~\n\nDepending on the type of data being processed; a synchronous,\nasynchronous, or parallel processing method may be ideal. Fetching data\nfrom multiple sources is suited for asynchronous or thread based\nparallel processing. Computational intensive tasks are suited for\nprocessor based parallel processing. And asynchronous processing is best\nsuited for debugging or low latency environments.\n\nriko is designed to support all of these paradigms using the same api.\nThis means switching between paradigms requires trivial code changes\nsuch as adding a yield statement or changing a keyword argument.\n\nUsing a series of examples, I will show each of these paradigms in\naction.\n",
  "duration": 1941,
  "language": "eng",
  "recorded": "2016-10-06",
  "related_urls": [
    "https://2016.za.pycon.org/talks/35/"
  ],
  "speakers": [
    "Reuben Cummings"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/oyM3CxtCjUQ/hqdefault.jpg",
  "title": "Stream processing made easy with riko",
  "videos": [
    {
      "type": "youtube",
      "url": "http://youtu.be/oyM3CxtCjUQ"
    },
    {
      "type": "archive",
      "url": "https://archive.org/details/pyconza2016-Stream_processing_made_easy_with_riko"
    }
  ]
}
