{
  "copyright_text": null,
  "description": "Las redes generativas antag\u00f3nicas (conocidas como GANs por sus siglas en ingl\u00e9s) son una de las tecnolog\u00edas m\u00e1s revolucionarias de los \u00faltimos a\u00f1os, no obstante han sido incluidas por Forbes en la lista de las mejores innovaciones de los \u00faltimos tres a\u00f1os e identificadas por MIT Tech Review como una de las tecnolog\u00edas m\u00e1s prometedoras en 2018.\n\nT\u00e9cnicamente pueden definirse como un tipo espec\u00edfico de red neuronal, concebida especialmente para imitar la redes neuronales cl\u00e1sicas, aquellas que a d\u00eda de hoy se conocen tambi\u00e9n como deep learning. Son capaces de aprender qu\u00e9 est\u00e1 modelando una red neuronal y generar datos v\u00e1lidos en su dominio desde cero. Esto significa que si una red neuronal categoriza im\u00e1genes de caras, una red generativa antag\u00f3nica asociada aprender\u00e1 a generar nuevas im\u00e1genes de caras, \u00a1de gente que no existe!\n\nUna de las consecuencias de tener dicha capacidad de imitaci\u00f3n es que se pueden configurar para enga\u00f1ar a las redes neuronales cl\u00e1sicas, generando datos que no es posible detectar como falsos. Este caso de uso tiene su lado oscuro, ya que el poder generar datos sint\u00e9ticos indistinguibles de los datos reales puede usarse para atacar redes neuronales que est\u00e9n integradas en sistemas cr\u00edticos y de seguridad. La t\u00e9cnica para explotar esta vulnerabilidad se ha bautizado como ataque antag\u00f3nico o adversarial attack.\n\nEn esta charla veremos primero una peque\u00f1a introducci\u00f3n sobre redes neuronales y explicaremos qu\u00e9 son las redes generativas antag\u00f3nicas. Despu\u00e9s veremos c\u00f3mo las redes generativas aprenden a imitar otras redes, hasta el punto de lograr enga\u00f1arlas, y por qu\u00e9 esto supone una vulnerabilidad en nuestros algoritmos de machine learning. Para ilustrar un ataque antag\u00f3nico usaremos un ejemplo en Python para enga\u00f1ar a un sistema de reconocimiento de im\u00e1genes. Por \u00faltimo discutiremos por qu\u00e9 este problema es importante y las enormes consecuencias que puede llegar a tener en nuestros sistemas de machine learning.\n",
  "duration": 1465,
  "language": "spa",
  "recorded": "2018-10-07",
  "related_urls": [
    {
      "label": "Conference schedule",
      "url": "https://2018.es.pycon.org/#schedule"
    }
  ],
  "speakers": [
    "Alicia P\u00e9rez",
    "Javier Ord\u00f3\u00f1ez"
  ],
  "tags": [],
  "thumbnail_url": "https://i.ytimg.com/vi/D2m9Ejx6S9k/maxresdefault.jpg",
  "title": "The bad guys in AI: atacando sistemas de machine learning",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=D2m9Ejx6S9k"
    }
  ]
}
