{
  "copyright_text": null,
  "description": "When machine learning models make decisions that affect people\u2019s lives,\nhow can you be sure those decisions are fair? When you build a machine\nlearning product, how can you be sure your product isn't biased? What\ndoes it even mean for an algorithm to be \u2018fair\u2019? As machine learning\nbecomes more prevalent in socially impactful domains like policing,\nlending, and education these questions take on a new urgency.\n\nIn this talk I\u2019ll introduce several common metrics which measure the\nfairness of model predictions. Next I\u2019ll relate these metrics to\ndifferent notions of fairness and show how the context in which a model\nor product is used determines which metrics (if any) are applicable. To\nillustrate this context-dependence I'll describe a case study of\nanonymized real-world data. Next, I'll highlight some open source tools\nin the Python ecosystem which address model fairness. Finally, I'll\nconclude by arguing that if your job involves building these kinds\nmodels or products then it is your responsibility to think about the\nanswers to these questions.\n",
  "duration": 1707,
  "language": "eng",
  "recorded": "2019-05-04T17:10:00",
  "related_urls": [
    {
      "label": "Conference schedule",
      "url": "https://us.pycon.org/2019/schedule/talks/"
    },
    {
      "label": "Conference slides (github)",
      "url": "https://github.com/PyCon/2019-slides"
    },
    {
      "label": "Conference slides (speakerdeck)",
      "url": "https://speakerdeck.com/pycon2019"
    },
    {
      "label": "Talk schedule",
      "url": "https://us.pycon.org/2019/schedule/presentation/201/"
    }
  ],
  "speakers": [
    "J. Henry Hinnefeld"
  ],
  "tags": [
    "talk"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/pYXz507ths0/maxresdefault.jpg",
  "title": "Measuring Model Fairness",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=pYXz507ths0"
    }
  ]
}
