{
  "copyright_text": null,
  "description": "In data science (in its all its variants) a significant part of an\nindividual\u2019s time is spent preparing data into a digestible format. In\ngeneral, a data science pipeline starts with the acquisition of raw data\nwhich is then manipulated through ETL processes and leads to a series of\nanalytics. Good data pipelines can be used to automate and schedule\nthese steps, help with monitoring tasks, and even to dynamically train\nmodels. On top of that, they make the analyses easier to reproduce and\nproductise.\n\nIn this workshop, you will learn how to migrate from \u2018scripts soups\u2019 (a\nset of scripts that should be run in a particular order) to robust,\nreproducible and easy-to-schedule data pipelines in Airflow. First, we\nwill learn how to write simple recurrent ETL pipelines. We will then\nintegrate logging and monitoring capabilities. And we will end using\nAirflow along with Jupyter Notebooks and paper mill to produce\nreproducible analytics reports.\n",
  "duration": 11875,
  "language": "eng",
  "recorded": "2019-05-01T09:00:00",
  "related_urls": [
    {
      "label": "Conference schedule",
      "url": "https://us.pycon.org/2019/schedule/talks/"
    },
    {
      "label": "Conference slides (speakerdeck)",
      "url": "https://speakerdeck.com/pycon2019"
    },
    {
      "label": "Conference slides (github)",
      "url": "https://github.com/PyCon/2019-slides"
    },
    {
      "label": "Talk schedule",
      "url": "https://us.pycon.org/2019/schedule/presentation/96/"
    }
  ],
  "speakers": [
    "Tania Allard"
  ],
  "tags": [
    "tutorial"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/n9_JjmHRtys/hqdefault.jpg",
  "title": "Building data pipelines in Python: Airflow vs scripts soup",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=n9_JjmHRtys"
    }
  ]
}
