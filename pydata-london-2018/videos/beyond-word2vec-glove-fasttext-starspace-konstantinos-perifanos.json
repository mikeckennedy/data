{
  "abstract": "Word embeddings, mappings from words to d-dimensional vectors, is a very\nconvenient and efficient way to extract semantic information from large\ncollections of textual or textual-like data.\n\nIn this talk we will have a closer look on the maths and the performance\nof different word embedding techniques, where the generated vector\nrepresentations are used as features for NLP tasks such as metaphor and\nsarcasm detection as well as applications beyond natural language, eg\nproduct similarity from clickstream data, source code e.t.c\n",
  "copyright_text": null,
  "description": "Word embeddings is a very convenient and efficient way to extract\nsemantic information from large collections of textual or textual-like\ndata. We will be presenting an exploration and comparison of the\nperformance of \"traditional\" embeddings techniques like word2vec and\nGloVe as well as fastText and StarSpace in NLP related problems such as\nmetaphor and sarcasm detection\n",
  "duration": 2323,
  "language": "eng",
  "recorded": "2018-04-28",
  "related_urls": [
    {
      "label": "Conference schedule",
      "url": "https://pydata.org/london2018/schedule/"
    }
  ],
  "speakers": [
    "Konstantinos Perifanos"
  ],
  "tags": [],
  "thumbnail_url": "https://i.ytimg.com/vi/6xPnEh_tJEc/maxresdefault.jpg",
  "title": "Beyond word2vec: GloVe, fastText, StarSpace.",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=6xPnEh_tJEc"
    }
  ]
}
