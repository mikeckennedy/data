{
  "copyright_text": null,
  "description": "It is common practice to test the performance of ML models, but it is not so common to test the reliability of the predictions. Training a model, test its performance and hoping that it will produce good quality predictions is not the right approach if we are concerned with reliable ML. Hence, in this talk, we will discuss the concept of conformal predictions which quantify quality in predictions.",
  "duration": 2138,
  "language": "eng",
  "recorded": "2019-07-13",
  "related_urls": [
    {
      "label": "Conference schedule",
      "url": "https://pydata.org/london2019/schedule/"
    },
    {
      "label": "Talk slides",
      "url": "https://www.slideshare.net/MariaIsabelNavarroJi/py-data19-final"
    }
  ],
  "speakers": [
    "Maria Navarro"
  ],
  "tags": [],
  "thumbnail_url": "https://i.ytimg.com/vi/r6bhm_A-YcQ/maxresdefault.jpg",
  "title": "Quantifying uncertainty in Machine Learning predictions",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=r6bhm_A-YcQ"
    }
  ]
}
