{
  "alias": "video/400/pycon-2011--large-scale-data-conditioning--amp--p",
  "category": "PyCon US 2011",
  "copyright_text": "Creative Commons Attribution-NonCommercial-ShareAlike 3.0",
  "description": "Large Scale Data Conditioning & Processing with Stackless Python and\nPypes\n\nPresented by Eric Gaumer\n\nPypes is a component oriented framework for designing dataflow\napplications. It uses Stackless Python to model components as\ncomputational entities that operate by sending and receiving messages.\nComponents are designed to process streams of data modeled as a series\nof messages which are exchanged asynchronously. Data streams are\ninitiated over an asynchronous REST interface.\n\nAbstract\n\nThere's been some recent momentum around data flow programming with a\nnumber of new frameworks having been released. This new found interest\nis due largely in part to the increasing amount of data being produced\nand consumed by applications. MapReduce has become a general topic of\ndiscussion for analytics over large data sets but it's increasingly\nevident that it's not a panacea.\n\nSimple batch processing tools like MapReduce and Hadoop are just not\npowerful enough in any one of the dimensions of the big data space that\nreally matters. One particular area where MapReduce falls short is in\nnear real-time search. It used to be common to run batch processing jobs\non a nightly basis which would index the days events, making them\nsearchable.\n\nGiven today's social dynamics, people have come to expect instant access\nto data as opposed to a daily digest. Batch oriented semantics are being\nsuperseded by event driven architectures that act on live, real-time\nstreams of data. This shift in paradigm has sparked new interest in\ndataflow concepts.\n\nDataflow frameworks promote the data to become the main concept behind\nany program. It becomes a matter of \"data-flow\" over \"control-flow\"\nwhere processes are just the way data is created, manipulated and\ndestroyed. This concept is well represented in the Unix operating system\nwhich pipes data between small single-purpose tools to produce more\nsophisticated applications.\n\nPypes is a dataflow framework that leverages Stackless Python to model\nprocesses as black box operations that communicate by sending and\nreceiving messages. These processes are naturally component oriented\nallowing them to be connected in different ways to form new\napplications. Components are inherently stateless making parallel\nprocessing relatively simple. Because a component is an abstraction of a\nStackless tasklet (true coroutines), expensive setups such as loading\nmachine learning models are done once during initialization and can then\nbe used throughout the life of the component. This is in contrast to\nMapReduce frameworks that typically incur this overhead each time the\nmap function is called or try to manage some sort of global state.\n\nOne aspect that differentiates Pypes from other dataflow frameworks is\nits \"push\" model. Unlike generator based solutions which pull data\nthrough the system, Pypes provides a RESTful interface that allows data\nto be pushed in. This allows Pypes to sit more natural as an event\ndriven middleware component in the context of a larger architecture. A\ndata push model also simplifies scalability since an entire cluster of\nnodes can be setup behind a load balancer which will then automatically\npartition the incoming data stream. Generator based \"pull models\" cannot\neasily partition data without somehow coordinating access to the data\nwhich involves global state.\n\nPypes was designed to be a highly scalable, event driven, dataflow\nscheduling and execution environment. Writing your own components is\nsimple and Pypes provides Paste templates for creating new projects.\nComponents are packaged as Python eggs and discovered automatically.\nThey can be wired together using a visual editor that runs in any HTML5\ncompliant browser. Pypes supports Directed Acyclic Graphs and data\nstreams are modeled as a series of JSON (dict) packets which support\nmeta-data at both the packet level and the field level.\n\nPypes also leverages the Python multiprocessing module to scale up. Data\narriving through the REST interface on any given node will be\ndistributed across parallel instances of the graph running on different\ncores/CPUs. Data submission is completely asynchronous.\n\nThis talk will provide a gentle introduction to the Pypes architecture\nand design.\n\nOutline:\n\n-  Brief intro to Stackless Python (benefits it provides)\n-  Control-Flow vs Data-Flow\n-  Preemptive vs Cooperative Scheduling\n-  The Topological Scheduler\n-  The REST API (Submitting Data - Asynchronous Web Service)\n-  Packet API: A unified data model with meta-data support\n-  Writing Custom Components - Paste templates and pluggable eggs\n-  Scale up - multiprocessing support\n-  Scale out - cloud friendly\n-  Questions\n\n",
  "duration": null,
  "id": 400,
  "language": "eng",
  "quality_notes": "",
  "recorded": "2011-03-11",
  "slug": "pycon-2011--large-scale-data-conditioning--amp--p",
  "speakers": [
    "Eric Gaumer"
  ],
  "summary": "",
  "tags": [
    "pycon",
    "pycon2011",
    "pypes",
    "stackless"
  ],
  "thumbnail_url": "https://archive.org/services/img/pyvideo_400___large-scale-data-conditioning-amp-processing-with-stackless-python-and-pypes",
  "title": "Large Scale Data Conditioning &amp; Processing with Stackless Python and Pypes",
  "videos": [
    {
      "type": "archive.org",
      "url": "https://archive.org/details/pyvideo_400___large-scale-data-conditioning-amp-processing-with-stackless-python-and-pypes"
    }
  ]
}
