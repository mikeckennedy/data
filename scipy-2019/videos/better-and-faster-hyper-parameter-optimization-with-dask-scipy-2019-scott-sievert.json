{
  "copyright_text": null,
  "description": "Nearly every machine learning model requires that the user specify certain parameters before training begins, aka \"hyper-parameters\". Finding the optimal set of hyper-parameters is often a time- and resource-consuming process. A recent breakthrough hyper-parameter optimization algorithm, Hyperband, can find high performing hyper-parameters with minimal training and has theoretical backing. This talk will provide intuition for Hyperband, explain it's use and why it's well-suited for Dask, a Python library that scales Python to larger datasets and more computational resources. Experiments find high performing hyper-parameters more quickly in the presence of parallel computational resources and with a deep learning model.",
  "duration": 1653,
  "language": "eng",
  "recorded": "2019-07-13",
  "related_urls": [
    {
      "label": "Conference schedule",
      "url": "https://www.scipy2019.scipy.org/confschedule"
    },
    {
      "label": "Talk slides",
      "url": "https://github.com/stsievert/talks"
    }
  ],
  "speakers": [
    "Tom Augspurger",
    "Matthew Rocklin",
    "Scott Sievert"
  ],
  "tags": [],
  "thumbnail_url": "https://i.ytimg.com/vi/x67K9FiPFBQ/maxresdefault.jpg",
  "title": "Better and Faster Hyper Parameter Optimization with Dask",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=x67K9FiPFBQ"
    }
  ]
}
