{
  "description": "Processing large datasets in R have been limited by the amount of\nmemory in the local system. To overcome the native R limitation,\nseveral cluster computing alternatives have recently emerged including\nApache Spark. In this session, we will discuss the architecture of\nSpark and introduce the SparkR library. We will work through examples\nof the API and discuss additional resources to learn more.\n\nIn this tutorial, we will focus on SparkR. The outline of the tutorial\nis as follows: - Introduction to cluster computing with Spark -\nGetting started with SparkR - Deep dive into SparkR DataFrame API -\nAdditional resources\n\nIn preparation for this tutorial please install.packages(\"SparkR\") in\nyour system.",
  "duration": 4671,
  "language": "eng",
  "recorded": "2016-09-14",
  "speakers": [
    "Zeydy Ortiz",
    "Rob Montalvo"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/iTzUOq30dHs/hqdefault.jpg",
  "title": "Scalable Data Science with Spark and R",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=iTzUOq30dHs"
    }
  ]
}
