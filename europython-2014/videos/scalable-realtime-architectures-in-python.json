{
  "alias": "video/2948/scalable-realtime-architectures-in-python",
  "category": "EuroPython 2014",
  "copyright_text": "http://creativecommons.org/licenses/by/3.0/",
  "description": "Increasingly we are interested in implementing highly scalable and fault\ntolerant realtime architectures such as the following:\n\n-  Realtime aggregation. This is the realtime analogue of working with\n   batched map-reduce in systems like Hadoop.\n\n-  Realtime dashboards. Continuously updated views on all your\n   customers, systems, and the like, without breaking a sweat.\n\n-  Realtime decision making. Given a set of input streams, policy on\n   what you like to do, and models learned by machine learning, optimize\n   a business process. One example includes autoscaling a set of\n   servers.\n\n(We use realtime in the soft sense: systems that are continuously\ncomputing on input streams of data and make a best effort to keep up; it\ncertainly does not imply hard realtime systems that strictly bound their\ncomputation times.)\n\nObvious tooling for such implementations include Storm (for event\nprocessing), Kafka (for queueing), and ZooKeeper (for tracking and\nconfiguration). Such components, written respectively in Clojure\n(Storm), Scala (Kafka), and Java (ZooKeeper), provide the desired\nscalability and reliability. But what may not be so obvious at first\nglance is that we can work with other languages, including Python, for\nthe application level of such architectures. (If so inclined, you can\nalso try reimplementing such components in Python, but why not use\nsomething that's been proven to be robust?)\n\nIn fact Python is likely a better language for the app level, given that\nit is concise, high level, dynamically typed, and has great libraries.\nNot to mention fun to write code in! This is especially true when we\nconsider the types of tasks we need to write: they are very much like\nthe data transformations and analyses we would have written of say a\nstandard Unix pipeline. And no one is going to argue that writing such a\nfilter in say Java is fun, concise, or even considerably faster in\nrunning time.\n\nSo let's look at how you might solve such larger problems. Given that it\nwas straightforward to solve a small problem, we might approach as\nfollows. Simply divide up larger problems in small one. For example,\nperhaps work with one customer at a time. And if failure is an ever\npresent reality, then simply ensure your code retries, just like you\nmight have re-run your pipeline against some input files.\n\nUnfortunately both require distributed coordination at scale. And\ndistributed coordination is challenging, especially for real systems,\nthat will break at scale. Just putting a box in your architecture\nlabeled **\"ZooKeeper\"** doesn't magically solve things, even if\nZooKeeper can be a very helpful part of an actual solution.\n\nEnter the Storm framework. While Storm certainly doesn't solve all\nproblems in this space, it can support many different types of realtime\narchitectures and works well with Python. In particular, Storm solves\ntwo key problems for you.\n\n**Partitioning**. Storm lets you partition streams, so you can break\ndown the size of your problem. But if the a node running your code\nfails, Storm will restart it. Storm also ensures such topology\ninvariants as the number of nodes (spouts and bolts in Storm's lingo)\nthat are running, making it very easy to recover from such failures.\n\nThis is where the cleverness really begins. What can you do if you can\nensure that **all the data** you need for a given continuously updated\ncomputation - what is the state of this customer's account? - can be put\nin **exactly one place**, then flow the supporting data through it over\ntime? We will look at how you can readily use such locality in your own\nPython code.\n\n**Retries**. Storm tracks success and failure of events being processed\nefficiently through a batching scheme and other cleverness. Your code\ncan then choose to retry as necessary. Although Storm also supports\nexactly-once event processing semantics, we will focus on the simpler\nmodel of at-least-once semantics. This means your code must tolerate\nretry, or in a word, is idempotent. But this is straightforward. We have\noften written code like the following:\n\n::\n\n    seen = set()\n    for record in stream:\n        k = uniquifier(record)\n        if k not in seen:\n           seen.add(k)\n           process(record)\n\nExcept of course that any such real usage has to ensure it doesn't\nattempt to store all observations (first, download the Internet! ;), but\nremoves them by implementing some sort of window or uses data structures\nlike HyperLogLog, as we will discuss.\n\nOne more aspect of reliability we will discuss is how to compose\nreliable systems out of reliable components; we will show how this can\nbe readily done with a real example of consuming Kafka and tracking\nconsumption progress in ZooKeeper.\n",
  "duration": null,
  "id": 2948,
  "language": "eng",
  "quality_notes": "",
  "recorded": "2014-07-25",
  "slug": "scalable-realtime-architectures-in-python",
  "speakers": [
    "Jim Baker"
  ],
  "summary": "This talk will focus on you can readily implement highly scalable and\nfault tolerant realtime architectures, such as dashboards, using Python\nand tools like Storm, Kafka, and ZooKeeper. We will focus on two related\naspects: composing reliable systems using at-least-once and idempotence\nsemantics and how to partition for locality.\n",
  "tags": [],
  "thumbnail_url": "https://i.ytimg.com/vi/Iw0MwPL2FCU/hqdefault.jpg",
  "title": "Scalable Realtime Architectures in Python",
  "videos": [
    {
      "length": 0,
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=Iw0MwPL2FCU"
    }
  ]
}
