{
  "description": "Speaker: Marianne Stecklina\n\nTrack:PyData\nLanguage models like BERT can capture general language knowledge and transfer it to new data and tasks. However, applying a pre-trained BERT to non-English text has limitations. Is training from scratch a good (and feasible) way to overcome them?\n\nRecorded at the PyConDE & PyData Berlin 2019 conference.\nhttps://pycon.de\n\nMore details at the conference page: https://de.pycon.org/program/YAJRGX\nTwitter:  https://twitter.com/pydataberlin\nTwitter:  https://twitter.com/pyconde",
  "language": "eng",
  "recorded": "2019-10-09",
  "related_urls": [
    {
      "label": "Conference Schedule",
      "url": "https://de.pycon.org/program/YAJRGX"
    }
  ],
  "speakers": [
    "Marianne Stecklina"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/-EvtAWr3GIo/hqdefault.jpg",
  "title": "Why you should (not) train your own BERT model for...",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=-EvtAWr3GIo"
    }
  ]
}
