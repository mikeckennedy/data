{
  "description": "Currently, some popular data processing frameworks such as Apache Spark\nconsider batch and stream processing jobs independently. The APIs across\ndifferent processing systems such as Apache Spark or Apache Flink are\nalso different. This forces the end user to learn a potentially new\nsystem every time. Apache Beam [1] addresses this problem by providing a\nunified programming model that can be used for both batch and streaming\npipelines. The Beam SDK allows the user to execute these pipelines\nagainst different execution engines. Currently, Apache Beam provides a\nJava and Python SDK.\n\nIn the talk, we start off by providing an overview of Apache Beam using\nthe Python SDK and the problems it tries to address from an end user\u2019s\nperspective. We cover the core programming constructs in the Beam model\nsuch as PCollections, ParDo, GroupByKey, windowing, and triggers. We\ndescribe how these constructs make it possible for pipelines to be\nexecuted in a unified fashion in both batch and streaming. Then we use\nexamples to demonstrate these capabilities. The examples showcase using\nBeam for stream processing and real time data analysis, and how Beam can\nbe used for feature engineering in some Machine Learning applications\nusing Tensorflow. Finally, we end with Beam\u2019s vision of creating runner\nand execution independent graphs using the Beam FnApi [2].\n\nApache Beam [1] is a top level Apache project and is completely open\nsource. The code for Beam can be found on Github [3].\n\n[1] https://beam.apache.org/ [2] http://s.apache.org/beam-fn-api [3]\nhttps://github.com/apache/beam",
  "language": "eng",
  "recorded": "2017-10-07",
  "speakers": [
    "Sourabh Bajaj"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/xcRmhrUKKSE/hqdefault.jpg",
  "title": "What, Where, When, How of Stream Processing",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=xcRmhrUKKSE"
    }
  ]
}
