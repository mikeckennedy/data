{
  "category": "DePy 2016",
  "copyright_text": "",
  "description": "In this talk we provide a user-friendly introduction to mathematical optimization for machine learning by essentially answering three important questions: (i) what is mathematical optimization, (ii) why should a machine learning researcher/practitioner learn it, and (iii) how does it actually work?\n\nEvery machine learning problem has parameters that must be tuned properly to ensure optimal learning. As a simple example consider the case of linear regression with one dimensional input, where the two parameters slope and intercept of the linear model are tuned by forming a 'cost function' - a continuous function in both parameters - that measures how well the linear model fits a dataset given a value for its slope and intercept. The proper tuning of these parameters via the cost function corresponds geometrically to finding the values for the parameters that make the cost function as small as possible or, in other words, \u2019minimize\u2019 the cost function. The tuning of these parameters is accomplished by a set of tools known collectively as mathematical optimization.\n\nMathematical optimization, as the formal study of how to properly minimize cost functions, is used not only in solving virtually every machine learning problem (regression, classification, clustering, etc.), but reasons in a variety of other fields including operations, logistics, and physics. As a result, a mere working knowledge of how to use existing pre-packaged solvers will not be adequate for any serious machine learning developer who wants to code-up their own implementation or tailor existing algorithms to a specific application.\n\nThe lion\u2019s share of this talk is dedicated to showing how to implement widely-used optimization schemes in Python. We plan to do so by introducing the concept of iterative methods and presenting two extremely popular iterative schemes: gradient descent and Newton\u2019s method. This will be followed by a discussion of stochastic gradient descent \u2013 a variant of gradient descent often referred to as the Backpropagation algorithm, most suitable for today\u2019s large datasets. Live Python demos will be run for all algorithms discussed here.\n\nThis talk is based on a forthcoming machine learning textbook (Machine Learning Refined; Cambridge University Press, 2016) co-authored by the speakers: Reza Borhani and Jeremy Watt (PhD, Computer Science, Northwestern University). This text has also been the source for a number of quarter length university courses on machine learning, deep learning, and numerical optimization for graduate and senior level undergraduate students. The speakers have also given/plan to give a number of tutorials on deep learning at major computer vision and AI conferences including CVPR, AAAI, ICIP, WACV, and more.",
  "duration": null,
  "id": 4733,
  "language": "eng",
  "quality_notes": "",
  "recorded": "2016-05-07",
  "slug": "mathematical-optimization-for-machine-learning",
  "speakers": [
    "Jeremy Watt",
    "Reza Borhani"
  ],
  "summary": "",
  "tags": [],
  "thumbnail_url": "https://img.youtube.com/vi/DYFcGgBBsD8/hqdefault.jpg",
  "title": "Mathematical Optimization for Machine Learning",
  "videos": [
    {
      "type": "youtube",
      "url": "http://youtu.be/DYFcGgBBsD8"
    }
  ]
}
