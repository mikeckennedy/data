{
  "description": "As the applications we build are increasingly driven by text, doing data\ningestion, management, loading, and preprocessing in a robust,\norganized, parallel, and memory-safe way can get tricky. This talk walks\nthrough the highs (a custom billion-word corpus!), the lows (segfaults,\n400 errors, pesky mp3s), and the new Python libraries we built to ingest\nand preprocess text for machine learning.\n\nWhile applications like Siri, Cortana, and Alexa may still seem like\nnovelties, language-aware applications are rapidly becoming the new\nnorm. Under the hood, these applications take in text data as input,\nparse it into composite parts, compute upon those composites, and then\nrecombine them to deliver a meaningful and tailored end result. The best\napplications use language models trained on *domain-specific corpora*\n(collections of related documents containing natural language) that\nreduce ambiguity and prediction space to make results more intelligible.\nHere's the catch: these corpora are huge, generally consisting of at\nleast hundreds of gigabytes of data inside of thousands of documents,\nand often more!\n\nIn this talk, we'll see how working with text data is substantially\ndifferent from working with numeric data, and show that ingesting a raw\ntext corpus in a form that will support the construction of a data\nproduct is no trivial task. For instance, when dealing with a text\ncorpus, you have to consider not only how the data comes in (e.g.\nrespecting rate limits, terms of use, etc.), but also where to store the\ndata and how to keep it organized. Because the data comes from the web,\nit's often unpredictable, containing not only text but audio files, ads,\nvideos, and other kinds of web detritus. Since the datasets are large,\nyou need to anticipate potential performance problems and ensure memory\nsafety through streaming data loading and multiprocessing. Finally, in\nanticipation of the machine learning components, you have to establish a\nstandardized method of transforming your raw ingested text into a corpus\nthat's ready for computation and modeling.\n\nIn this talk, we'll explore many of the challenges we experienced along\nthe way and introduce two Python packages that make this work a bit\neasier: `Baleen <https://pypi.python.org/pypi/baleen/0.3.3>`__ and\n`Minke <https://github.com/bbengfort/minke>`__. Baleen is a package for\ningesting formal natural language data from the discourse of\nprofessional and amateur writers, like bloggers and news outlets, in a\ncategorized fashion. Minke extends Baleen with a library that performs\nparallel data loading, preprocessing, normalization, and keyphrase\nextraction to support machine learning on a large-scale custom corpus.\n",
  "duration": 1877,
  "recorded": "2017-05-19",
  "speakers": [
    "Rebecca Bilbro"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/j1DdGX2d9BE/hqdefault.jpg",
  "title": "Building A Gigaword Corpus: Lessons on Data Ingestion, Management, and Processing for NLP",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=j1DdGX2d9BE"
    }
  ]
}
