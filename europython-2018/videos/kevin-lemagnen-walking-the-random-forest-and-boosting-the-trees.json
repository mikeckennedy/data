{
  "copyright_text": "Creative Commons Attribution license (reuse allowed)",
  "description": "Deep Learning is all the rage, but ensemble models are still in the\ngame. With libraries such as the recent and performant LightGBM, the\nKaggle superstar XGboost or the classic Random Forest from scikit-learn,\nensembles models are a must-have in a data scientist\u2019s toolbox. They\u2019ve\nbeen proven to provide good performance on a wide range of problems, and\nare usually simpler to tune and interpret. This talk focuses on two of\nthe most popular tree-based ensemble models. You will learn about Random\nForest and Gradient Boosting, relying respectively on bagging and\nboosting. This talk will attempt to build a bridge between the theory of\nensemble models and their implementation in Python.\n\nNotebook:\n``https://github.com/klemag/europython2018_walking_the_random_forest``\n",
  "duration": 1450,
  "language": "eng",
  "recorded": "2018-07-27",
  "related_urls": [
    {
      "label": "Conference schedule",
      "url": "https://ep2018.europython.eu/p3/schedule/ep2018/"
    }
  ],
  "speakers": [
    "Kevin Lemagnen"
  ],
  "tags": [],
  "thumbnail_url": "https://i.ytimg.com/vi/SbeLVSyO9_E/maxresdefault.jpg",
  "title": "Walking the Random Forest and boosting the trees",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=SbeLVSyO9_E"
    }
  ]
}
