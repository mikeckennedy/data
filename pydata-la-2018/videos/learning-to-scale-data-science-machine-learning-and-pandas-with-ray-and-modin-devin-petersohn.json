{
  "abstract": "Exercise 1: Simple Data Parallel Example\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe goal of this exercise is to show how to run simple tasks in\n'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n\nparallel.\n\nAttendees will:\n\n-  Learn about ``@ray.remote`` decorator.\n\n-  Learn about how to submit a function remotely.\n\n-  Learn how to use futures in Ray.\n\n-  Learn about ``ray.get(...)``.\n\n-  Learn how to access the Ray UI.\n\nExercise 2: Parallel Data Processing with Task Dependencies\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe goal of this exercise is to show how to pass object IDs into remote\n'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n\nfunctions to encode dependencies between tasks.\n\nAttendees will:\n\n-  Advance their knowledge about the futures in Ray.\n\n-  Learn how to create remote task dependencies in Ray.\n\nExercise 3: Tree Reduce\n^^^^^^^^^^^^^^^^^^^^^^^\n\nThe goal of this exercise is to show how to implement a tree reduce in\n''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n\nRay by passing object IDs into remote functions to encode dependencies\nbetween tasks.\n\nAttendees will learn how to write their own tree reduce in Ray.\n\nExercise 4: Nested Parallelism\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe goal of this exercise is to show how to create nested tasks by\n''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n\ncalling a remote function inside of another remote function.\n\nAttendees will learn how to write a parallel hyperparameter sweep in\nRay.\n\nExercise 5: Handling Slow Tasks\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe goal of this exercise is to show how to use ``ray.wait`` to avoid\n'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n\nwaiting for slow tasks.\n\nAttendees will:\n\n-  Learn how to use ``ray.wait`` to block on remote call completion\n\n-  Learn how to use the result of a subset of remote tasks\n\nExercise 6: Process Tasks in Order of Completion\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe goal of this exercise is to show how to use ``ray.wait`` to process\n'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n\ntasks in the order that they finish.\n\nAttendees will:\n\n-  Advance their knowledge of the uses of ``ray.wait``\n\n-  Learn how to process tasks in order of completion (as opposed to\n   order of submission)\n\nExercise 7: Introducing Actors\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe goal of this exercise is to show how to create an actor and how to\n''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n\ncall actor methods.\n\nAttendees will:\n\n-  Learn about the Actor API in Ray\n\n-  Learn how to create Actors in Ray\n\n-  Learn how to call methods on Actors in Ray\n\nExercise 8: Actor Handles\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe goal of this exercise is to show how to pass around actor handles.\n''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n\nAttendees will:\n\n-  Learn about Actor Handles and how to use them\n\n-  Learn how to pass Actor Handles to a remote task\n\n-  Learn how to call an Actor method from a remote task\n\nExercise 9: Speed up Serialization\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe goal of this exercise is to illustrate how to speed up serialization\n''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n\nby using ``ray.put``.\n\nAttendees will:\n\n-  Learn about some of the best practices when using ``ray.put``\n\n-  Learn how to reuse serialized objects for multiple tasks\n\nExercise 10: Using the GPU API\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe goal of this exercise is to show how to use GPUs with remote\n''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n\nfunctions and actors.\n\nAttendees will:\n\n-  Learn how to use Ray with GPUs\n\n-  Learn how to create an Actor that has GPUs allocated to it\n\nExercise 11: Custom Resources\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe goal of this exercise is to show how to use custom resources\n''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n\nAttendees will:\n\n-  Learn how to have more control over resources in Ray with custom\n   resources\n\n-  Learn how to schedule tasks with hardware requirements on specific\n   machines with custom resources\n\nExercise 12: Pass Neural Net Weights Between Processes\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe goal of this exercise is to show how to send neural network weights\n'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n\nbetween workers and the driver.\n\nAttendees will:\n\n-  Advance their knowledge on general principles covered in previous\n   exercises\n\n-  Apply these exercises to solve a real-world problem\n\n-  Learn how to pass TensorFlow weights between tasks\n\nExercise 13: Modin, Learning to Increase the Speed of Pandas Workflows\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nwith One Line of Code Change\n\nThe goal of this exercise is to show how to use Modin to speed up Pandas\n''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n\nworkflows and interact with data\n\nAttendees will:\n\n-  Learn how to speed up Pandas workflows with Modin\n\n-  Learn about the API compatibility with Pandas\n\n-  Learn about the speed of ``read_csv``\n\n-  Learn about the current limitations of Modin\n",
  "copyright_text": null,
  "description": "In this tutorial, attendees will learn how to use Ray to scale their new\nand existing Python code. It will cover the Ray system architecture,\nexample applications, GPU support, and best practices. It will also\ninclude material for more comprehensive exercises. Attendees will also\nreceive a tutorial on Modin, and how Pandas workflows can be scaled by\nchanging a single line of code.\n",
  "duration": 1723,
  "language": "eng",
  "recorded": "2018-10-21",
  "related_urls": [
    {
      "label": "schedule",
      "url": "https://pydata.org/la2018/schedule/"
    }
  ],
  "speakers": [
    "Devin Petersohn"
  ],
  "tags": [],
  "thumbnail_url": "https://i.ytimg.com/vi/1sYuMFtMZQc/maxresdefault.jpg",
  "title": "Learning to Scale Data Science, Machine Learning, and Pandas with Ray and Modin",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=1sYuMFtMZQc"
    }
  ]
}
