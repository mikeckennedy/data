{
  "copyright_text": "Creative Commons Attribution license (reuse allowed)",
  "description": "Recurrent Neural Networks (RNNs) have become famous over time due to\ntheir property of retaining internal memory. These neural nets are\nwidely used in recognizing patterns in sequences of data, like numerical\ntimer series data, images, handwritten text, spoken words, genome\nsequences, and much more. Since these nets possess memory, there is a\ncertain analogy that we can make to the human brain in order to learn\nhow RNNs work. RNNs can be thought of as a network of neurons with\nfeedback connections, unlike feedforward connections which exist in\nother types of Artificial Neural Networks.\n\nThe flow of talk will be as follows: - Self Introduction - Introduction\nto Deep Learning - Artificial Neural Networks (ANNs) - Diving DEEP into\nRecurrent Neural Networks (RNNs) - Comparing Feedforward Networks with\nFeedback Networks - Quick walkthrough: Implementing RNNs using Python\n(Keras) - Understanding Backpropagation Through Time (BPTT) and\nVanishing Gradient Problem - Towards more sophisticated RNNs: Gated\nRecurrent Units (GRUs)/Long Short-Term Memory (LSTMs) - End of talk -\nQuestions and Answers Session\n",
  "duration": 2378,
  "language": "eng",
  "recorded": "2018-07-25",
  "related_urls": [
    {
      "label": "Conference schedule",
      "url": "https://ep2018.europython.eu/p3/schedule/ep2018/"
    }
  ],
  "speakers": [
    "Anmol Krishan Sachdeva"
  ],
  "tags": [],
  "thumbnail_url": "https://i.ytimg.com/vi/bSdBG7hToOg/maxresdefault.jpg",
  "title": "Understanding and Implementing Recurrent Neural Networks using Python",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=bSdBG7hToOg"
    }
  ]
}
